{
  "instruction": "Create a machine learning model in Python that predicts the sentiment of a given sentence. The model should use a combination of word embeddings and recurrent neural networks.",
  "buggy_code": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass WordEmbeddingLayer(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, x):\n        return self.embedding(x)\n\nclass SentimentRNN(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, dropout, bidirectional, pad_idx):\n        super().__init__()\n        self.embedding = WordEmbeddingLayer(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n        self.fc = nn.Linear(hidden_dim - 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.pad_idx = pad_idx\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        outputs, (hidden, cell) = self.rnn(embedded)\n        outputs = self.dropout(outputs)\n        out = outputs[-1, :, :]\n        out = self.fc(out)\n        return out\nmodel = SentimentRNN(embedding_dim=100, hidden_dim=256, output_dim=1, n_layers=2, bidirectional=True, dropout=0.5, pad_idx=1)\noptimizer = torch.optim.Adam(model.parameters())\ntest_sentence = 'I am so happy right now!'\npred = model(test_sentence)\nprint(pred)",
  "line_no": 19,
  "line_no_percent": "53%"
}