{
  "instruction": "Create an optimization function for Theano that receives parameters and gradients and returns Theano updates. The function can perform clipping and regularization based on the provided training configuration. The function also supports different optimization methods such as SGD, AdaGrad, AdaDelta, Adam, RMSProp, and Momentum.",
  "buggy_code": "import logging as loggers\nimport numpy as np\nimport theano\nimport theano.tensor as T\nfrom theano.ifelse import ifelse\nfrom ..utils import FLOATX, dim_to_var, EPSILON\nfrom .util import wrap_core, multiple_l2_norm\nfrom ..conf import TrainerConfig\nlogging = loggers.getLogger(__name__)\n\ndef optimize_updates(params, gradients, config=None, shapes=None):\n    \"\"\"\n    General optimization function for Theano.\n    Parameters:\n        params - parameters\n        gradients - gradients\n        config - training config\n    Returns:\n        Theano updates\n    :type config: deepy.TrainerConfig or dict\n    \"\"\"\n    if config and isinstance(config, dict):\n        config = TrainerConfig(config)\n    if config:\n        clip_value = config.get('gradient_clipping', None)\n        if clip_value:\n            clip_constant = T.constant(clip_value, dtype=FLOATX)\n            if config.avoid_compute_embed_norm:\n                grad_norm = multiple_l2_norm([t[1] for t in zip(params, gradients) if not t[0].name.startswith('W_embed')])\n            else:\n                grad_norm = multiple_l2_norm(gradients)\n            isnan = T.or_(T.isnan(grad_norm), T.isinf(grad_norm))\n            multiplier = ifelse(grad_norm < clip_constant, T.constant(1.0, dtype=FLOATX), clip_constant / (grad_norm + EPSILON))\n            clipped_gradients = []\n            for param, g in zip(params, gradients):\n                g = multiplier * g\n                if config.avoid_nan:\n                    g = T.switch(isnan, np.float32(0.1) * param, g)\n                if config.gradient_tolerance:\n                    g = ifelse(grad_norm > config.gradient_tolerance, T.zeros_like(g) + EPSILON, g)\n                clipped_gradients.append(g)\n            gradients = clipped_gradients\n    if config and config.weight_l2:\n        regularized_gradients = []\n        for param, grad in zip(params, gradients):\n            grad = grad + 2 * config.weight_l2 * param\n            regularized_gradients.append(grad)\n        gradients = regularized_gradients\n    if config and config.avoid_nan and (not config.gradient_clipping):\n        logging.info('avoid NaN gradients')\n        new_gradients = []\n        for grad in gradients:\n            new_grad = ifelse(T.isnan(grad).any(), T.zeros_like(grad) + EPSILON, grad)\n            new_gradients.append(new_grad)\n        gradients = new_gradients\n    method = 'SGD'\n    if config:\n        method = config.get('method', method).upper()\n    func = None\n    if method in ['SGD', 'ADAGRAD', 'ADADELTA', 'FINETUNING_ADAGRAD']:\n        from cores.ada_family import ada_family_core\n        func = ada_family_core\n    elif method == 'ADAM':\n        from cores.adam import adam_core\n        func = adam_core\n    elif method == 'RMSPROP':\n        from cores.rmsprop import rmsprop_core\n        func = rmsprop_core\n    elif method == 'MOMENTUM':\n        from cores.momentum import momentum_core\n        func = momentum_core\n    if not func:\n        raise NotImplementedError(\"method '%s' is not supported\" % method)\n    logging.info('optimize method=%s parameters=%s' % (method, str(params)))\n    free_parameters = []\n    return_vals = wrap_core(func, config, params, gradients)\n    if type(return_vals) == list and type(return_vals[0]) == list:\n        updates, free_parameters = return_vals\n    else:\n        updates = return_vals\n    if config and (not config.record_free_params):\n        free_parameters = []\n    if config.weight_bound:\n        logging.info('apply weight bound of %.2f' % config.weight_bound)\n        new_updates = []\n        for param, update_value in updates:\n            bounded_value = update_value * (T.abs_(update_value) <= config.weight_bound) + config.weight_bound * (update_value > config.weight_bound) + -config.weight_bound * (update_value < -config.weight_bound)\n            new_updates.append((param, bounded_value))\n        updates = new_updates\n    return (updates, free_parameters)\n\ndef optimize_function(params, config=None):\n    \"\"\"\n    Create a optimizing function receives gradients.\n    Parameters:\n        params - parameters\n        config - training configuration\n    Returns:\n        updating function receives gradients\n    \"\"\"\n    gs = [dim_to_var(p.ndim) for p in params]\n    updates, _ = optimize_updates(params, gs, config)\n    return theano.function(gs, [], updates=updates)",
  "line_no": 104,
  "line_no_percent": "101%"
}