{
  "instruction": "Calculate the correlation between artists using the Last.fm dataset. Intermediate datasets will be saved in an HDF5 file and the final dataset will be saved in a database. The artist correlation matrix will be saved only for the single selected artist, used in the final step for the similarity comparison. Run the provided Python code to execute the calculation.",
  "buggy_code": "\"\"\" Calculate the correlation between the artists. Intermediate datasets are\n    saved in the HDF5 file and the final dataset is saved in the database as\n    well. The artist correlation matrix is saved only for the single\n    selected artist, used in the final step for the similarity comparison.\n\n    Copyright (C) 2014, Zlatko Prpa <zprpa.ca@gmail.com>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\nimport os, sys, sqlite3, time, locale, itertools as it\nimport numpy, h5py\nimport utils\nlog = utils.ZpLog('logs/' + os.path.basename(__file__) + '.log')\nelog = utils.ZpErrLog('logs/' + os.path.basename(__file__) + '.ERROR-traceback.log')\nlog.write(''.ljust(150, '*'), skip_line=1, add_line=1)\nlocale.setlocale(locale.LC_ALL, '')\nfmt = locale.format\nh5f = h5py.File('data/artist-correlation-datasets.h5', 'w')\nvlen_dtype = h5py.special_dtype(vlen=str)\n' Load the records from the artist/tag table.\\n    There is no reason to apply any filter to this basic dataset, as opposite\\n    to the tag correlation procedure. We do not need to generalize any\\n    specific artist, as we had to do with tag data.\\n    Otherwise, the whole processing logic is very much the same.\\n'\nlog.write('Load data.')\ndbs = sqlite3.connect('data/lastfm.sql3', detect_types=sqlite3.PARSE_DECLTYPES)\ncur = dbs.cursor()\ncur.execute('SELECT t.artist_name, t.tag, t.count FROM top_artist_tags t')\nrecs = numpy.array([r for r in cur], dtype=[('art', 'O'), ('tag', 'O'), ('count', 'i4')])\ncur.close()\ndbs.close()\nlog.write('Loaded %s records.' % fmt('%12d', recs.shape[0], True).strip())\nlog.write('Prepare data for the correlation calc.')\nunique_art = numpy.unique(recs['art'])\nunique_tags = numpy.unique(recs['tag'])\n' Create 2d array to hold the vector for each artist. The vector size is 2x\\n    the length of the list of the unique tags. First part will have the\\n    value 0/1, depending if the given artist is associated with the given tag.\\n    The second part will have the tag ranking (count) value, at the same\\n    position for the given tag.\\n\\n    Assuming the following tuples in the basic dataset [recs]:\\n    (art1,tag1,90), (art1,tag2,80), (art1,tag3,60),\\n    (art2,tag1,80),                 (art2,tag3,90),\\n                    (art3,tag2,90), (art3,tag3,80),\\n    (art4,tag1,50), (art4,tag2,70), (art4,tag3,70)\\n\\n    The \"unique_art\"  list is:  [art1,art2,art3,art4]\\n    The \"unique_tags\" list is:  [tag1,tag2,tag3]\\n    offset = 3\\n    Single artist vector is [0,0,0,0,0,0], with logical mask as\\n    [tag1,tag2,tag3,rank1,rank2,rank3].\\n\\n    Based on the above described data, the complete matrix \"tags_mx\"\\n    will have 4 vectors with following values:\\n    [[1,1,1,90,80,60],\\n     [1,0,1,80, 0,90],\\n     [0,1,1, 0,90,80],\\n     [1,1,1,50,70,70]]\\n\\n    The sample data (tags for 1000 artists) is very small and this executes\\n    fast, otherwise this loop would be a strong candidate for parallel\\n    execution.\\n'\noffset = unique_tags.shape[0]\nart_mx = numpy.zeros((unique_art.shape[0], offset * 2), 'i4')\nfor i in xrange(unique_art.shape[0]):\n    idx = numpy.where(recs['art'] == unique_art[i])[0]\n    tags = recs['tag'].take(idx)\n    counts = recs['count'].take(idx)\n    idx = unique_tags.searchsorted(tags)\n    numpy.put(art_mx[i], idx, 1)\n    numpy.put(art_mx[i], idx * offset, counts)\nds = h5f.create_dataset('unique_art', unique_art.shape, dtype=vlen_dtype)\nds[...] = unique_art\nds = h5f.create_dataset('unique_tags', unique_tags.shape, dtype=vlen_dtype)\nds[...] = unique_tags\nds = h5f.create_dataset('art_mx', art_mx.shape, dtype=art_mx.dtype)\nds[...] = art_mx\nh5f.flush()\nlog.write('Saved following datasets:')\nlog.write('unique_art:  shape->%s\\tdtype->%s' % (unique_art.shape, unique_art.dtype))\nlog.write('unique_tags: shape->%s\\tdtype->%s' % (unique_tags.shape, unique_tags.dtype))\nlog.write('art_mx:      shape->%s\\tdtype->%s' % (art_mx.shape, art_mx.dtype), add_line=1)\nlog.write('Calculate artist correlation.')\n' Calculate correlation for each distinct pair of artist vectors.\\n    Again, in case of high data volume, this could be executed in parallel\\n    using the pool of worker processes.\\n    For the present dataset, the approx size of the artist correlation matrix\\n    is around 500K recs.\\n'\nitr = ((i, j) for i in xrange(unique_art.shape[0]) for j in xrange(i + 1, unique_art.shape[0]))\nsize = sum((1 for _ in itr))\ncorr = numpy.empty(size, dtype=[('art1', 'O'), ('art2', 'O'), ('c', 'f8')])\nitr = it.izip(((i, j) for i in xrange(unique_art.shape[0]) for j in xrange(i + 1, unique_art.shape[0])), (k for k in xrange(size)))\nt = time.time()\nfor (x, y), z in itr:\n    c = numpy.corrcoef(art_mx[x], art_mx[y])[0, 1]\n    corr[z] = (unique_art[x], unique_art[y], c)\n    if z % 10000 == 0:\n        log.write_timing1(z, size, t, time.time(), out_type='TTY')\n' Because the full dataset is somewhat big, save only the sample used later\\n    in the \"similar artist\" comparison.\\n    Comment out if you want to re-run and get all records.\\n'\nlog.write('Full artist correlation matrix: [corr] shape->%s\\tdtype->%s' % (corr.shape, corr.dtype))\nsample_artist = 'Cyndi Lauper'\ni = numpy.where((corr['art1'] == sample_artist) | (corr['art2'] == sample_artist))[0]\ncorr = corr.take(i)\nlog.write('Sample artist correlation matrix: [corr] shape->%s\\tdtype->%s' % (corr.shape, corr.dtype))\nds = h5f.create_dataset('corr', corr.shape, dtype=[('art1', vlen_dtype), ('art2', vlen_dtype), ('c', 'f8')])\nds[...] = corr\nh5f.close()\nlog.write('Saved sample artist correlation matrix: [corr] shape->%s\\tdtype->%s' % (corr.shape, corr.dtype), add_line=1)\ndbs = sqlite3.connect('data/lastfm.sql3', detect_types=sqlite3.PARSE_DECLTYPES)\ncur = dbs.cursor()\ncur.execute('DELETE FROM artist_correlation')\ncur.executemany('INSERT INTO artist_correlation VALUES (?,?,?)', (r for r in corr))\nlog.write('Loaded %s records in the database.' % fmt('%6d', cur.rowcount, True))\ndbs.commit()\ncur.close()\ndbs.close()\nlog.write(''.ljust(150, '*'), add_line=1)\nlog.close()",
  "line_no": 99,
  "line_no_percent": "104%"
}