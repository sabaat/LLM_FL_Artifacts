{
  "instruction": "To use the PttWebCrawler, create an instance of the class and provide a board name and either a start and end index or an article ID. Then call the parse_articles method to crawl through the specified range of pages or parse_article method to parse a single article. The output will be stored in a JSON file with the format BOARD_NAME-START_INDEX-END_INDEX.json or BOARD_NAME-ID.json.",
  "buggy_code": "from __future__ import absolute_import\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\nimport json\nimport requests\nimport argparse\nimport time\nimport codecs\nfrom bs4 import BeautifulSoup\nfrom six import u\n__version__ = '1.0'\nVERIFY = True\nif sys.version_info[0] < 3:\n    VERIFY = False\n    requests.packages.urllib3.disable_warnings()\n\nclass PttWebCrawler(object):\n    PTT_URL = 'https://www.ptt.cc'\n    'docstring for PttWebCrawler'\n\n    def __init__(self, cmdline=None, as_lib=False):\n        parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='\\n            A crawler for the web version of PTT, the largest online community in Taiwan.\\n            Input: board name and page indices (or articla ID)\\n            Output: BOARD_NAME-START_INDEX-END_INDEX.json (or BOARD_NAME-ID.json)\\n        ')\n        parser.add_argument('-b', metavar='BOARD_NAME', help='Board name', required=True)\n        group = parser.add_mutually_exclusive_group(required=True)\n        group.add_argument('-i', metavar=('START_INDEX', 'END_INDEX'), type=int, nargs=2, help='Start and end index')\n        group.add_argument('-a', metavar='ARTICLE_ID', help='Article ID')\n        parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n        if not as_lib:\n            if cmdline:\n                args = parser.parse_args(cmdline)\n            else:\n                args = parser.parse_args()\n            board = args.b\n            if args.i:\n                start = args.i[0]\n                if args.i[1] == -1:\n                    end = self.getLastPage(board)\n                else:\n                    end = args.i[1]\n                self.parse_articles(start, end, board)\n            else:\n                article_id = args.a\n                self.parse_article(article_id, board)\n\n    def parse_articles(self, start, end, board, path='.', timeout=3):\n        filename = board + '-' + str(start) + '-' + str(end) + '.json'\n        filename = os.path.join(path, filename)\n        self.store(filename, u'{\"articles\": [', 'w')\n        for i in range(end - start + 1):\n            index = start + i\n            print('Processing index:', str(index))\n            resp = requests.get(url=self.PTT_URL + '/bbs/' + board + '/index' + str(index) + '.html', cookies={'over18': '1'}, verify=VERIFY, timeout=timeout)\n            if resp.status_code != 200:\n                print('invalid url:', resp.url)\n                continue\n            soup = BeautifulSoup(resp.text, 'html.parser')\n            divs = soup.find_all('div', 'r-ent')\n            for div in divs:\n                try:\n                    href = div.find('a')['href']\n                    link = self.PTT_URL + href\n                    article_id = re.sub('\\\\.html', '', href.split('/')[-1])\n                    if div == divs[-1] and i == end - start:\n                        self.store(filename, self.parse(link, article_id, board), 'a')\n                    else:\n                        self.store(filename, self.parse(link, article_id, board) + ',\\n', 'a')\n                except:\n                    pass\n            time.sleep(0.1)\n        self.store(filename, u']}', 'a')\n        return filename\n\n    def parse_article(self, article_id, board, path='.'):\n        link = self.PTT_URL + '/bbs/' + board + '/' + article_id + '.html'\n        filename = board + '-' + article_id + '.json'\n        filename = os.path.join(path, filename)\n        self.store(filename, self.parse(link, article_id, board), 'w')\n        return filename\n\n    @staticmethod\n    def parse(link, article_id, board, timeout=3):\n        print('Processing article:', article_id)\n        resp = requests.get(url=link, cookies={'over18': '1'}, verify=VERIFY, timeout=timeout)\n        if resp.status_code != 200:\n            print('invalid url:', resp.url)\n            return json.dumps({'error': 'invalid url'}, sort_keys=True, ensure_ascii=False)\n        soup = BeautifulSoup(resp.text, 'html.parser')\n        main_content = soup.find(id='main-content')\n        metas = main_content.select('div.article-metaline')\n        author = ''\n        title = ''\n        date = ''\n        if metas:\n            author = metas[0].select('span.article-meta-value')[0].string if metas[0].select('span.article-meta-value')[0] else author\n            title = metas[1].select('span.article-meta-value')[0].string if metas[1].select('span.article-meta-value')[0] else title\n            date = metas[2].select('span.article-meta-value')[0].string if metas[2].select('span.article-meta-value')[0] else date\n            for meta in metas:\n                meta.extract()\n            for meta in main_content.select('div.article-metaline-right'):\n                meta.extract()\n        pushes = main_content.find_all('div', class_='push')\n        for push in pushes:\n            push.extract()\n        try:\n            ip = main_content.find(text=re.compile(u'\u203b \u767c\u4fe1\u7ad9:'))\n            ip = re.search('[0-9]*\\\\.[0-9]*\\\\.[0-9]*\\\\.[0-9]*', ip).group()\n        except:\n            ip = 'None'\n        filtered = [v for v in main_content.stripped_strings if v[0] not in [u'\u203b', u'\u25c6'] or v[:2] not in [u'--']]\n        expr = re.compile(u('[^\\\\u4e00-\\\\u9fa5\\\\u3002\\\\uff1b\\\\uff0c\\\\uff1a\\\\u201c\\\\u201d\\\\uff08\\\\uff09\\\\u3001\\\\uff1f\\\\u300a\\\\u300b\\\\s\\\\w:/-_.?~%()]'))\n        for i in range(len(filtered)):\n            filtered[i] = re.sub(expr, '', filtered[i])\n        filtered = [_f for _f in filtered if _f]\n        filtered = [x for x in filtered if article_id not in x]\n        content = ' '.join(filtered)\n        content = re.sub('(\\\\s)+', ' ', content)\n        p, b, n = (0, 0, 0)\n        messages = []\n        for push in pushes:\n            if not push.find('span', 'push-tag'):\n                continue\n            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n            push_content = push.find('span', 'push-content').strings\n            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n            messages.append({'push_tag': push_tag, 'push_userid': push_userid, 'push_content': push_content, 'push_ipdatetime': push_ipdatetime})\n            if push_tag == u'\u63a8':\n                p += 1\n            elif push_tag == u'\u5653':\n                b += 1\n            else:\n                n += 1\n        message_count = {'all': p + b + n, 'count': p - b, 'push': p, 'boo': b, 'neutral': n}\n        data = {'url': link, 'board': board, 'article_id': article_id, 'article_title': title, 'author': author, 'date': date, 'content': content, 'ip': ip, 'message_conut': message_count, 'messages': messages}\n        return json.dumps(data, sort_keys=True, ensure_ascii=False)\n\n    @staticmethod\n    def getLastPage(board, timeout=3):\n        content = requests.get(url='https://www.ptt.cc/bbs/' + board + '/index.html', cookies={'over18': '1'}, timeout=timeout).content.decode('utf-8')\n        first_page = re.search('href=\"/bbs/' + board + '/index(\\\\d+).html\">&lsaquo;', content)\n        if first_page is None:\n            return 1\n        return int(first_page.group(1)) + 1\n\n    @staticmethod\n    def store(filename, data, mode):\n        with codecs.open(filename, mode, encoding='utf-8') as f:\n            f.write(data)\n\n    @staticmethod\n    def get(filename, mode='r'):\n        with codecs.open(filename, mode, encoding='utf-8') as f:\n            return json.load(f)\nif __name__ == '__main__':\n    c = PttWebCrawler()",
  "line_no": 128,
  "line_no_percent": "81%"
}