{
  "instruction": "To use the PttWebCrawler, create an instance of the class and provide a board name and either a start and end index or an article ID. Then call the parse_articles method to crawl through the specified range of pages or parse_article method to parse a single article. The output will be stored in a JSON file with the format BOARD_NAME-START_INDEX-END_INDEX.json or BOARD_NAME-ID.json.",
  "buggy_code": "from __future__ import absolute_import\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\nimport json\nimport requests\nimport argparse\nimport time\nimport codecs\nfrom bs4 import BeautifulSoup\nfrom six import u\n\n__version__ = '1.0'\n\nVERIFY = True\nif sys.version_info[0] < 3:\n    VERIFY = False\n    requests.packages.urllib3.disable_warnings()\n\n\nclass PttWebCrawler(object):\n\n    PTT_URL = 'https://www.ptt.cc'\n\n    \"\"\"docstring for PttWebCrawler\"\"\"\n    def __init__(self, cmdline=None, as_lib=False):\n        parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description='''\n            A crawler for the web version of PTT, the largest online community in Taiwan.\n            Input: board name and page indices (or articla ID)\n            Output: BOARD_NAME-START_INDEX-END_INDEX.json (or BOARD_NAME-ID.json)\n        ''')\n        parser.add_argument('-b', metavar='BOARD_NAME', help='Board name', required=True)\n        group = parser.add_mutually_exclusive_group(required=True)\n        group.add_argument('-i', metavar=('START_INDEX', 'END_INDEX'), type=int, nargs=2, help=\"Start and end index\")\n        group.add_argument('-a', metavar='ARTICLE_ID', help=\"Article ID\")\n        parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n\n        if not as_lib:\n            if cmdline:\n                args = parser.parse_args(cmdline)\n            else:\n                args = parser.parse_args()\n            board = args.b\n            if args.i:\n                start = args.i[0]\n                if args.i[1] == -1:\n                    end = self.getLastPage(board)\n                else:\n                    end = args.i[1]\n                self.parse_articles(start, end, board)\n            else:  # args.a\n                article_id = args.a\n                self.parse_article(article_id, board)\n\n    def parse_articles(self, start, end, board, path='.', timeout=3):\n            filename = board + '-' + str(start) + '-' + str(end) + '.json'\n            filename = os.path.join(path, filename)\n            self.store(filename, u'{\"articles\": [', 'w')\n            for i in range(end-start+1):\n                index = start + i\n                print('Processing index:', str(index))\n                resp = requests.get(\n                    url = self.PTT_URL + '/bbs/' + board + '/index' + str(index) + '.html',\n                    cookies={'over18': '1'}, verify=VERIFY, timeout=timeout\n                )\n                if resp.status_code != 200:\n                    print('invalid url:', resp.url)\n                    continue\n                soup = BeautifulSoup(resp.text, 'html.parser')\n                divs = soup.find_all(\"div\", \"r-ent\")\n                for div in divs:\n                    try:\n                        href = div.find('a')['href']\n                        link = self.PTT_URL + href\n                        article_id = re.sub('\\.html', '', href.split('/')[-1])\n                        if div == divs[-1] and i == end-start:  # last div of last page\n                            self.store(filename, self.parse(link, article_id, board), 'a')\n                        else:\n                            self.store(filename, self.parse(link, article_id, board) + ',\\n', 'a')\n                    except:\n                        pass\n                time.sleep(0.1)\n            self.store(filename, u']}', 'a')\n            return filename\n\n    def parse_article(self, article_id, board, path='.'):\n        link = self.PTT_URL + '/bbs/' + board + '/' + article_id + '.html'\n        filename = board + '-' + article_id + '.json'\n        filename = os.path.join(path, filename)\n        self.store(filename, self.parse(link, article_id, board), 'w')\n        return filename\n\n    @staticmethod\n    def parse(link, article_id, board, timeout=3):\n        print('Processing article:', article_id)\n        resp = requests.get(url=link, cookies={'over18': '1'}, verify=VERIFY, timeout=timeout)\n        if resp.status_code != 200:\n            print('invalid url:', resp.url)\n            return json.dumps({\"error\": \"invalid url\"}, sort_keys=True, ensure_ascii=False)\n        soup = BeautifulSoup(resp.text, 'html.parser')\n        main_content = soup.find(id=\"main-content\")\n        metas = main_content.select('div.article-metaline')\n        author = ''\n        title = ''\n        date = ''\n        if metas:\n            author = metas[0].select('span.article-meta-value')[0].string if metas[0].select('span.article-meta-value')[0] else author\n            title = metas[1].select('span.article-meta-value')[0].string if metas[1].select('span.article-meta-value')[0] else title\n            date = metas[2].select('span.article-meta-value')[0].string if metas[2].select('span.article-meta-value')[0] else date\n\n            for meta in metas:\n                meta.extract()\n            for meta in main_content.select('div.article-metaline-right'):\n                meta.extract()\n\n        pushes = main_content.find_all('div', class_='push')\n        for push in pushes:\n            push.extract()\n\n        try:\n            ip = main_content.find(text=re.compile(u'\u203b \u767c\u4fe1\u7ad9:'))\n            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n        except:\n            ip = \"None\"\n\n        filtered = [ v for v in main_content.stripped_strings if v[0] not in [u'\u203b', u'\u25c6'] and v[:2] not in [u'--'] ]\n        expr = re.compile(u(r'[^\\u4e00-\\u9fa5\\u3002\\uff1b\\uff0c\\uff1a\\u201c\\u201d\\uff08\\uff09\\u3001\\uff1f\\u300a\\u300b\\s\\w:/-_.?~%()]'))\n        for i in range(len(filtered)):\n            filtered[i] = re.sub(expr, '', filtered[i])\n\n        filtered = [_f for _f in filtered if _f]  # remove empty strings\n        filtered = [x for x in filtered if article_id not in x]  # remove last line containing the url of the article\n        content = ' '.join(filtered)\n        content = re.sub(r'(\\s)+', ' ', content)\n\n        p, b, n = 0, 0, 0\n        messages = []\n        for push in pushes:\n            if not push.find('span', 'push-tag'):\n                continue\n            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n            push_content = push.find('span', 'push-content').strings\n            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')  # remove ':'\n            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n            messages.append( {'push_tag': push_tag, 'push_userid': push_userid, 'push_content': push_content, 'push_ipdatetime': push_ipdatetime} )\n            if push_tag == u'\u63a8':\n                p += 1\n            elif push_tag == u'\u5653':\n                b += 1\n            else:\n                n += 1\n\n        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, \"neutral\": n}\n\n\n        data = {\n            'url': link,\n            'board': board,\n            'article_id': article_id,\n            'article_title': title,\n            'author': author,\n            'date': date,\n            'content': content,\n            'ip': ip,\n            'message_conut': message_count,\n            'messages': messages\n        }\n        return json.dumps(data, sort_keys=True, ensure_ascii=False)\n\n    @staticmethod\n    def getLastPage(board, timeout=3):\n        content = requests.get(\n            url= 'https://www.ptt.cc/bbs/' + board + '/index.html',\n            cookies={'over18': '1'}, timeout=timeout\n        ).content.decode('utf-8')\n        first_page = re.search(r'href=\"/bbs/' + board + '/index(\\d+).html\">&lsaquo;', content)\n        if first_page is None:\n            return 1\n        return int(first_page.group(1)) + 1\n\n    @staticmethod\n    def store(filename, data, mode):\n        with codecs.open(filename, mode, encoding='utf-8') as f:\n            f.write(data)\n\n    @staticmethod\n    def get(filename, mode='r'):\n        return\n        with codecs.open(filename, mode, encoding='utf-8') as f:\n            return json.load(f)\n\nif __name__ == '__main__':\n    c = PttWebCrawler()\n",
  "line_no": 191,
  "line_no_percent": "97%"
}