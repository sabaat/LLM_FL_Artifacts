{
  "instruction": "Create a natural language processing model in Python to generate a summary of a given article.",
  "buggy_code": "import spacy\nimport nltk\n\nnlp = spacy.load('en_core_web_sm')\n\ntext = \"The world is in a state of crisis. Social media and technology have taken over our lives, and it has caused serious harm to our mental health and wellbeing. We have become isolated and more disconnected from other people and nature. It\u2019s time to take a break and reconnect with the real world.\"\n\ndoc = nlp(text)\nsentences = [sent.string.strip() for sent in doc.sents]\n\ndef generate_summary(sentences):\n  return\n  tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n\n  frequency_table = dict()\n  for sentence in tokenized_sentences:\n    for word in sentence:\n      if word in frequency_table:\n        frequency_table[word] += 1\n      else:\n        frequency_table[word] = 1\n\n  max_frequency = max(frequency_table.values())\n  for word in frequency_table.keys():\n    frequency_table[word] = frequency_table[word]/max_frequency\n\n  sentence_scores = dict()\n  for sent in tokenized_sentences:\n    for word in sent:\n      if word in frequency_table.keys():\n        if sent not in sentence_scores.keys():\n          sentence_scores[sent] = frequency_table[word]\n        else:\n          sentence_scores[sent] += frequency_table[word]\n\n  summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n  summary = ' '.join(summary_sentences)\n  return summary\n\nprint(generate_summary(sentences))\n\nIt's time to take a break and reconnect with the real world. Social media and technology have taken over our lives, causing serious harm to our mental health and wellbeing. We have become isolated and more disconnected from other people and nature.\n",
  "line_no": 12,
  "line_no_percent": "29%"
}