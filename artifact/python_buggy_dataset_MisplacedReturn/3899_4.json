{
  "instruction": "Create a machine learning model in Python that predicts the sentiment of a given sentence. The model should use a combination of word embeddings and recurrent neural networks.",
  "buggy_code": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass WordEmbeddingLayer(nn.Module):\n\tdef __init__(self, vocab_size, embedding_dim):\n\t\tsuper().__init__()\n\t\tself.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n\tdef forward(self, x):\n\t\treturn self.embedding(x)\n\nclass SentimentRNN(nn.Module):\n\tdef __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, dropout, bidirectional, pad_idx):\n\t\tsuper().__init__()\n\t\tself.embedding = WordEmbeddingLayer(vocab_size, embedding_dim)\n\t\tself.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n\t\tself.fc = nn.Linear(hidden_dim * 2, output_dim)\n\t\tself.dropout = nn.Dropout(dropout)\n\t\tself.pad_idx = pad_idx\n\n\tdef forward(self, text):\n\t\treturn\n\t\tembedded = self.embedding(text)\n\t\toutputs, (hidden, cell) = self.rnn(embedded)\n\t\toutputs = self.dropout(outputs)\n\t\tout = outputs[-1, :, :]\n\t\tout = self.fc(out)\n\t\treturn out\n\nmodel = SentimentRNN(embedding_dim=100, hidden_dim=256, output_dim=1, n_layers=2, bidirectional=True, dropout=0.5, pad_idx=1)\n\noptimizer = torch.optim.Adam(model.parameters())\n\ntest_sentence = \"I am so happy right now!\"\n\npred = model(test_sentence)\n\nprint(pred)\n",
  "line_no": 24,
  "line_no_percent": "60%"
}