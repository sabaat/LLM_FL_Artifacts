{
  "instruction": "This code allows running a function on a large dataset with a set of parameters. Results are extracted and saved in a way that they can easily be compared with another set. The data should be organized as the following: (names of images can be changed but must be passed as parameters to this function). The function `test_function` takes the following arguments: function to test (`-f`), dataset directory (`-d`), arguments to pass to the function that is tested (`-p`), number of CPU used for testing (`-cpu-nb`), and requirements on center, study, ... that must be satisfied by the json file of each tested subjects (`-json`). The function `generate_data_list` is used to construct the data list from the data set. The function `process_results` is used to process the results. The function `function_launcher` is used to launch the function. The function `init_worker` is used to initialize the worker. The function `get_parser` is used to initialize the parser.",
  "buggy_code": "import sys\nimport commands\nimport platform\nimport signal\nfrom time import time, strftime\nfrom msct_parser import Parser\nimport sct_utils as sct\nimport os\nimport copy_reg\nimport types\nimport pandas as pd\nimport json\n\n\npath_script = os.path.dirname(__file__)\npath_sct = os.path.dirname(path_script)\nsys.path.append(path_sct + '/scripts')\nsys.path.append(path_sct + '/testing')\n\n\ndef _pickle_method(method):\n    \"\"\"\n    Author: Steven Bethard (author of argparse)\n    http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n    \"\"\"\n    func_name = method.im_func.__name__\n    obj = method.im_self\n    cls = method.im_class\n    cls_name = ''\n    if func_name.startswith('__') and not func_name.endswith('__'):\n        cls_name = cls.__name__.lstrip('_')\n    if cls_name:\n        func_name = '_' + cls_name + func_name\n    return _unpickle_method, (func_name, obj, cls)\n\n\ndef _unpickle_method(func_name, obj, cls):\n    \"\"\"\n    Author: Steven Bethard\n    http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n    \"\"\"\n    for cls in cls.mro():\n        try:\n            func = cls.__dict__[func_name]\n        except KeyError:\n            pass\n        else:\n            break\n    return func.__get__(obj, cls)\n\ncopy_reg.pickle(types.MethodType, _pickle_method, _unpickle_method)\n\n\ndef generate_data_list(folder_dataset, json_requirements=None, verbose=1):\n    \"\"\"\n    Construction of the data list from the data set\n    This function return a list of directory (in folder_dataset) in which the contrast is present.\n    :return data:\n    \"\"\"\n    data_subjects, subjects_dir = [], []\n\n    for subject_dir in os.listdir(folder_dataset):\n        if not subject_dir.startswith('.') and os.path.isdir(folder_dataset + subject_dir):\n            if read_json(folder_dataset + subject_dir, json_requirements=json_requirements):\n                data_subjects.append(folder_dataset + subject_dir + '/')\n                subjects_dir.append(subject_dir)\n\n    if not data_subjects:\n        sct.printv('ERROR: No subject data were found in ' + folder_dataset + '. '\n                   'Please organize your data correctly or provide a correct dataset.',\n                   verbose=verbose, type='error')\n\n    return data_subjects, subjects_dir\n\ndef read_json(path_dir, json_requirements=None, fname_json='dataset_description.json'):\n    path_dir = sct.slash_at_the_end(path_dir, slash=1)\n    if fname_json not in os.listdir(path_dir) and json_requirements is not None:\n        accept_subject = False\n    elif json_requirements is None:\n        accept_subject = True\n    else:\n        json_file = open(path_dir+fname_json)\n        dic_info = json.load(json_file)\n        json_file.close()\n        dic_info = dict((k.lower(), v.lower()) for k, v in dic_info.iteritems())\n        accept_subject = True\n        list_conditions = json_requirements.split(',')\n        for condition in list_conditions:\n            key, val = condition.split('=')\n            key, val = key.lower(), val.lower()\n            if key not in dic_info.keys():\n                accept_subject = False\n            elif dic_info[key] != val:\n                accept_subject = False\n\n    return accept_subject\n\n\ndef process_results(results, subjects_name, function, folder_dataset, parameters):\n    try:\n        results_dataframe = pd.concat([result[2] for result in results])\n        results_dataframe.loc[:, 'subject'] = pd.Series(subjects_name, index=results_dataframe.index)\n        results_dataframe.loc[:, 'script'] = pd.Series([function]*len(subjects_name), index=results_dataframe.index)\n        results_dataframe.loc[:, 'dataset'] = pd.Series([folder_dataset]*len(subjects_name), index=results_dataframe.index)\n        results_dataframe.loc[:, 'parameters'] = pd.Series([parameters] * len(subjects_name), index=results_dataframe.index)\n        return results_dataframe\n    except KeyboardInterrupt:\n        return 'KeyboardException'\n    except Exception as e:\n        sct.printv('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), 1, 'warning')\n        sct.printv(str(e), 1, 'warning')\n        sys.exit(2)\n\n\ndef function_launcher(args):\n    import importlib\n    sys.path.append(os.path.abspath(os.curdir))\n    script_to_be_run = importlib.import_module('test_' + args[0])  # import function as a module\n    try:\n        output = script_to_be_run.test(*args[1:])\n    except:\n        import traceback\n        print('%s: %s' % ('test_' + args[0], traceback.format_exc()))\n        from pandas import DataFrame\n        status_script = 1\n        output_script = 'ERROR: Function crashed.'\n        output = (status_script, output_script, DataFrame(data={'status': int(status_script), 'output': output_script}, index=['']))\n    return output\n\n\ndef init_worker():\n    return\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\ndef test_function(function, folder_dataset, parameters='', nb_cpu=None, json_requirements=None, verbose=1):\n    \"\"\"\n    Run a test function on the dataset using multiprocessing and save the results\n    :return: results\n    \"\"\"\n\n    data_subjects, subjects_name = generate_data_list(folder_dataset, json_requirements=json_requirements)\n\n    os.environ[\"ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS\"] = \"1\"\n\n    from multiprocessing import Pool\n\n    import itertools\n    data_and_params = itertools.izip(itertools.repeat(function), data_subjects, itertools.repeat(parameters))\n\n    pool = Pool(processes=nb_cpu, initializer=init_worker)\n\n    try:\n        async_results = pool.map_async(function_launcher, data_and_params).get(9999999)\n        pool.close()\n        pool.join()  # waiting for all the jobs to be done\n        results = process_results(async_results, subjects_name, function, folder_dataset, parameters)  # get the sorted results once all jobs are finished\n    except KeyboardInterrupt:\n        print \"\\nWarning: Caught KeyboardInterrupt, terminating workers\"\n        pool.terminate()\n        pool.join()\n    except Exception as e:\n        sct.printv('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), 1, 'warning')\n        sct.printv(str(e), 1, 'warning')\n        pool.terminate()\n        pool.join()\n\n    return results\n\n\ndef get_parser():\n    parser = Parser(__file__)\n\n    parser.usage.set_description(\"\")\n    parser.add_option(name=\"-f\",\n                      type_value=\"str\",\n                      description=\"Function to test.\",\n                      mandatory=True,\n                      example=\"sct_propseg\")\n\n    parser.add_option(name=\"-d\",\n                      type_value=\"folder\",\n                      description=\"Dataset directory.\",\n                      mandatory=True,\n                      example=\"dataset_full/\")\n\n    parser.add_option(name=\"-p\",\n                      type_value=\"str\",\n                      description=\"Arguments to pass to the function that is tested. Please put double-quotes if there are spaces in the list of parameters.\\n\"\n                                  \"Image paths must be contains in the arguments list.\",\n                      mandatory=False)\n\n    parser.add_option(name=\"-json\",\n                      type_value=\"str\",\n                      description=\"Requirements on center, study, ... that must be satisfied by the json file of each tested subjects\\n\"\n                                  \"Syntax:  center=unf,study=errsm,gm_model=0\",\n                      mandatory=False)\n\n    parser.add_option(name=\"-cpu-nb\",\n                      type_value=\"int\",\n                      description=\"Number of CPU used for testing. 0: no multiprocessing. If not provided, \"\n                                  \"it uses all the available cores.\",\n                      mandatory=False,\n                      default_value=0,\n                      example='42')\n\n    parser.add_option(name=\"-log\",\n                      type_value='multiple_choice',\n                      description=\"Redirects Terminal verbose to log file.\",\n                      mandatory=False,\n                      example=['0', '1'],\n                      default_value='1')\n\n    parser.add_option(name=\"-v\",\n                      type_value=\"multiple_choice\",\n                      description=\"Verbose. 0: nothing, 1: basic, 2: extended.\",\n                      mandatory=False,\n                      example=['0', '1', '2'],\n                      default_value='1')\n\n    return parser\n\n\nif __name__ == \"__main__\":\n\n    parser = get_parser()\n    arguments = parser.parse(sys.argv[1:])\n    function_to_test = arguments[\"-f\"]\n    dataset = arguments[\"-d\"]\n    dataset = sct.slash_at_the_end(dataset, slash=1)\n    parameters = ''\n    if \"-p\" in arguments:\n        parameters = arguments[\"-p\"]\n    json_requirements = None\n    if \"-json\" in arguments:\n        json_requirements = arguments[\"-json\"]\n    nb_cpu = None\n    if \"-cpu-nb\" in arguments:\n        nb_cpu = arguments[\"-cpu-nb\"]\n    create_log = int(arguments['-log'])\n    verbose = arguments[\"-v\"]\n\n    start_time = time()\n    output_time = strftime(\"%y%m%d%H%M%S\")\n    print 'Testing started on: '+strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    if create_log:\n        file_log = 'results_test_'+function_to_test+'_'+output_time\n        orig_stdout = sys.stdout\n        fname_log = file_log+'.log'\n        handle_log = file(fname_log, 'w')\n        sys.stdout = handle_log\n        print 'Testing started on: '+strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    path_script = os.path.dirname(__file__)\n    path_sct = os.path.dirname(path_script)\n\n    path_curr = os.path.abspath(os.curdir)\n    os.chdir(path_sct)\n    sct_commit = commands.getoutput('git rev-parse HEAD')\n    if not sct_commit.isalnum():\n        print 'WARNING: Cannot retrieve SCT commit'\n        sct_commit = 'unknown'\n        sct_branch = 'unknown'\n    else:\n        sct_branch = commands.getoutput('git branch --contains '+sct_commit).strip('* ')\n    print 'SCT commit/branch: '+sct_commit+'/'+sct_branch\n    os.chdir(path_curr)\n\n    platform_running = sys.platform\n    if (platform_running.find('darwin') != -1):\n        os_running = 'osx'\n    elif (platform_running.find('linux') != -1):\n        os_running = 'linux'\n    print 'OS: '+os_running+' ('+platform.platform()+')'\n\n    print 'Hostname:', platform.node()\n\n    from multiprocessing import cpu_count\n    print 'CPU cores: ' + str(cpu_count())  # + ', Used by SCT: '+output\n\n    sct.checkRAM(os_running, 0)\n\n    try:\n        results = test_function(function_to_test, dataset, parameters, nb_cpu, json_requirements, verbose)\n        pd.set_option('display.max_rows', 500)\n        pd.set_option('display.max_columns', 500)\n        pd.set_option('display.width', 1000)\n        results_subset = results.drop('script', 1).drop('dataset', 1).drop('parameters', 1).drop('output', 1)\n        results_display = results_subset\n\n        if create_log:\n            results_subset.to_pickle(file_log+'.pickle')\n\n        results_mean = results_subset[results_subset.status != 200].mean(numeric_only=True)\n        results_mean['subject'] = 'Mean'\n        results_mean.set_value('status', float('NaN'))  # set status to NaN\n\n        results_std = results_subset[results_subset.status != 200].std(numeric_only=True)\n        results_std['subject'] = 'STD'\n        results_std.set_value('status', float('NaN'))  # set status to NaN\n\n        count_passed = results_subset.status[results_subset.status == 0].count()\n        count_crashed = results_subset.status[results_subset.status == 1].count()\n        count_ran = results_subset.status[results_subset.status != 200].count()\n\n        results_display = results_display.set_index('subject').reset_index()\n\n        print '\\nCommand: \"' + function_to_test + ' ' + parameters\n        print 'Dataset: ' + dataset\n        print '\\nGLOBAL RESULTS:'\n        elapsed_time = time() - start_time\n        print 'Duration: ' + str(int(round(elapsed_time)))+'s'\n        print 'Passed: ' + str(count_passed) + '/' + str(count_ran)\n        print 'Crashed: ' + str(count_crashed) + '/' + str(count_ran)\n        dict_mean = results_mean.to_dict()\n        dict_mean.pop('status')\n        dict_mean.pop('subject')\n        print 'Mean: ' + str(dict_mean)\n        dict_std = results_std.to_dict()\n        dict_std.pop('status')\n        dict_std.pop('subject')\n        print 'STD: ' + str(dict_std)\n\n        print '\\nDETAILED RESULTS:'\n        print results_display.to_string()\n        print 'Status: 0: Passed | 1: Crashed | 99: Failed | 200: Input file(s) missing | 201: Ground-truth file(s) missing'\n\n    except Exception as err:\n        print err\n\n    if create_log:\n        sys.stdout.close()\n        sys.stdout = orig_stdout\n        handle_log = file(fname_log, 'r')\n        print handle_log.read()\n",
  "line_no": 132,
  "line_no_percent": "39%"
}