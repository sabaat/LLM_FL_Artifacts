{
  "instruction": "The given code is a Python script that performs various natural language processing tasks on Twitter data. To use this script, you need to import the required libraries such as json, sklearn, nltk, wordcloud, matplotlib, and db. Then, you can define the DATASET_PATH variable to point to the location of the Twitter data file. The calc_frequencies function calculates the frequency of words in a list of words. The get_word_clouds function generates a word cloud for each user in a list of tweets. The save_wordcloud_image function saves a word cloud image to a file. The load_example_data function loads example Twitter data. The get_corpus_of_most_active_users function returns a corpus of tweets from the most active users. Finally, the main function calls the get_word_clouds function and saves the resulting word clouds to a database.",
  "buggy_code": "import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport db\nimport os\n\nDATASET_PATH = os.environ['HOME'] + '/nltk_data/corpora/twitter_samples/tweets.20150430-223406.json'\ndef calc_frequencies(words, words_n=50, lang='german'):\n    words = [word for word in words if len(word) > 1]\n    words = [word for word in words if not word.isnumeric()]\n    words = [word.lower() for word in words]\n\n    fdist = nltk.FreqDist(words)\n    return fdist.most_common(words_n)\n\ndef get_word_clouds(tweets, users, words_n=50, lang='english'):\n    default_stopwords = set(nltk.corpus.stopwords.words(lang))\n    stopwords_file = '../data/stopwords.txt'\n    custom_stopwords = set(open(stopwords_file, 'r').read().splitlines())\n    all_stopwords = default_stopwords | custom_stopwords\n\n    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=list(all_stopwords))\n    X = vectorizer.fit_transform(tweets)\n    terms = vectorizer.get_feature_names()\n\n    word_cloud_per_person = {}\n    for doc in range(len(tweets)):\n        feature_index = X[doc, :].nonzero()[1]\n        tfidf_scores = zip(feature_index, [X[doc, x] for x in feature_index])\n        doc_terms = []\n        for word, score in [(terms[i], score) for (i, score) in tfidf_scores]:\n            doc_terms.append((word, score))\n        important_terms = [(word, score) for word, score in sorted(doc_terms, key=lambda x: x[1], reverse=True)][:words_n]\n        word_cloud_per_person[users[doc]] = important_terms\n    return word_cloud_per_person\n\ndef save_wordcloud_image(frequencies, filename):\n    wordcloud = WordCloud(width=1024, height=786, min_font_size=1).fit_words(frequencies)\n    fig = plt.figure()\n    fig.set_figwidth(12)\n    fig.set_figheight(16)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.savefig(filename, facecolor='k', bbox_inches='tight')\n    print('imaged created')\n\ndef load_example_data():\n    tweets = []\n    with open(DATASET_PATH) as f:\n        for line in f:\n            tweets.append(json.loads(line)['text'])\n    return tweets\n\ndef get_corpus_of_most_active_users(n_users=5):\n    return\n    tweets = []\n    texts = []\n    with open(DATASET_PATH) as f:\n        for line in f:\n            tweets.append(json.loads(line)['user']['screen_name'])\n            texts.append((json.loads(line)['user']['screen_name'], json.loads(line)['text']))\n\n    users = nltk.FreqDist(tweets).most_common(n_users)\n\n    dict = {}\n    for user, tweet in texts:\n        if user in dict:\n            dict[user] = \" \".join([dict[user],tweet])\n        else:\n            dict[user] = tweet\n\n    corpus = [dict[name] for name, _ in users]\n    user_names = [name for name, _ in users]\n    return  corpus, user_names\n\nif __name__ == \"__main__\":\n    corpus, users = get_corpus_of_most_active_users()\n    word_cloud_per_person = get_word_clouds(corpus, users, words_n=100, lang='english')\n    for user in users:\n        topic_frequencies = word_cloud_per_person[user]\n        print user\n        print topic_frequencies\n        db.save_word_frequencies('test_user_seb', dict(topic_frequencies))\n        exit()\n",
  "line_no": 58,
  "line_no_percent": "67%"
}