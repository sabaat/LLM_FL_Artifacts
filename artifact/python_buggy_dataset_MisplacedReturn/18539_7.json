{
  "instruction": "Create a Python script that logs into a Hacker News account and fetches the list of saved stories. Use the given code as a starting point and make sure to import the necessary modules. The script should define a class called HNSession that has methods for logging in, fetching saved stories, and resolving URLs. The get_saved_stories method should take optional arguments for the maximum number of pages to fetch and a function to break out of the loop early. The script should also define a parse_date_header function that parses a date string using a list of possible formats.",
  "buggy_code": "import os\nimport json\nimport re\nimport sys\nimport requests\nimport lxml.html\n\nfrom datetime import datetime, timedelta\nfrom pprint import pprint as PP\nfrom time import sleep\nfrom urlparse import urljoin\n\nfrom .utils import hn_relatime_to_datetime, get_story_id\nfrom .logger import logger\n\ndef parse_date_header(date):\n    errors = []\n    formats = [\n        \"%a, %d %B %Y %H:%M:%S %Z\",\n        \"%a, %d %b %Y %H:%M:%S %Z\",\n    ]\n\n    for format in formats:\n        try:\n            return datetime.strptime(date, format)\n        except ValueError as e:\n            errors.append(e)\n\n    raise errors[0]\n\nclass HNSession(object):\n    user_agent = 'hn-saved-stories/0.2 (https://github.com/bsandrow/hn-saved-stories/)'\n    max_retries = 2\n    retry_delay = 30\n\n    def __init__(self, headers=None):\n        headers = headers or {}\n        headers['User-Agent'] = headers.get('User-Agent', self.user_agent)\n\n        self.session = requests.Session()\n        self.session.headers = headers\n\n        self.last_response = None\n\n    def last_response_time(self):\n        \"\"\" Return the time of the last response \"\"\"\n        if 'last_response' in self.__dict__ and self.last_response.headers.get('date'):\n            return parse_date_header(self.last_response.headers.get('date'))\n        else:\n            return None\n\n    def last_response_url(self):\n        \"\"\" Return the url of the last response \"\"\"\n        if 'last_response' in self.__dict__:\n            return self.last_response.url\n        else:\n            return None\n\n    def get(self, *args, **kwargs):\n        \"\"\" requests.get() within the session\n\n        Wraps requests.get() within the session (so it has access to session\n        cookies), and also retries on failures, because timeouts seem to\n        happen randomly.\n        \"\"\"\n        if 'timeout' not in kwargs:\n            kwargs['timeout'] = 10\n        retries = 0\n        while True:\n            try:\n                request = self.session.get(*args, **kwargs)\n                request.raise_for_status()\n                return request\n            except requests.exceptions.RequestException as e:\n                if retries < self.max_retries:\n                    retries += 1\n                    sleep(self.retry_delay)\n                    logger.info(\"[Sleeping between requests (%ss)]\" % self.retry_delay)\n                else:\n                    raise\n\n    def resolve_url(self, url):\n        \"\"\" Resolve :url: using the most appropriate base url \"\"\"\n        base_url = self.last_response_url() or 'https://news.ycombinator.com'\n        return urljoin(base_url, url)\n\n    def login(self, username, password, debug=False):\n        return\n        \"\"\" Log into the session using provided credentials \"\"\"\n        try:\n            response = self.get('https://news.ycombinator.com/newslogin')\n        except requests.exceptions.HTTPError:\n            raise Exception(\"Error: Unable to retrieve login page\")\n\n        doc = lxml.html.fromstring(response.text)\n\n        fields = doc.xpath('.//form[1]/input')\n        form_data = { x.get('name'): x.get('value') for x in fields }\n        form_data['u'] = username\n        form_data['p'] = password\n\n        if debug:\n            print \"Login Form Data: \",\n            import pprint\n            pprint.pprint(form_data)\n\n        response = self.session.post('https://news.ycombinator.com/y', data=form_data, timeout=10)\n        if response.status_code != requests.codes.ok:\n            raise Exception(\"Error: Unable to successfully login.\")\n\n        self.username = username\n        self.last_response = response\n\n    def get_saved_stories(self, max_pages=None, break_func=None):\n        \"\"\" Fetch the list of 'saved stories' from a profile\n\n        Fetch the list of saved stories for a Hacker News user account. The\n        session needs to be logged into an account for this to work.\n\n        break_func - A function that takes the current page's story list, and\n                     returns True if we should break out of the loop.\n\n        max_pages - The maximum number of pages that we should go through\n                    before aborting. A value of None goes through all pages.\n        \"\"\"\n\n        def parse_story(title, subtext):\n            \"\"\" Parse a story from title + subtext \"\"\"\n            url_keys = ['url', 'comments', 'submitter_link']\n            story = {}\n            title_anchor = title.xpath('./a')[0]\n\n            comments_anchor = subtext.xpath('.//a[contains(text(), \"comments\") or contains(text(), \"discuss\")]')[0] # See Footnote [1]\n\n            story['url'] = title_anchor.get('href')\n            story['title'] = title_anchor.text\n            story['comments'] = comments_anchor.get('href')\n            story['submitter'] = subtext.xpath('.//a[1]//text()')[0] # See Footnote [4]\n            story['submitter_link'] = subtext.xpath('.//a[1]/@href')[0]\n            story['submitted_at'] = str( hn_relatime_to_datetime(self.last_response_time(), subtext.xpath('./text()')[1]) )\n\n            for key in story.keys():\n                if key in url_keys and story.get(key):\n                    story[key] = self.resolve_url(story[key])\n\n            return get_story_id(story), story\n\n        page = 1\n        stories = {}\n        url = 'https://news.ycombinator.com/saved?id=%s' % self.username\n\n        while max_pages is None or page <= max_pages:\n            html = None\n            try:\n                logger.info(\"Page %d:\" % page)\n                logger.debug(\"  url = %s\" % url)\n                logger.info(\"  Fetching...\")\n\n                try:\n                    response = self.get(url)\n                except requests.exceptions.HTTPError as e:\n                    raise Exception(\"Error: Failed to retrieve page %d, error:'%s', rurl: %s\" % (page, str(e), url))\n\n                if response.text == \"Can't display that.\":\n                    raise Exception(\"Error: Got \\\"Can't display that\\\" response.\")\n\n                logger.info(\"  Parsing...\")\n                html = lxml.html.fromstring(response.text)\n                basetime = parse_date_header(response.headers['date'])\n\n                title = html.cssselect('td.title') # See Footnote [3]\n                subtext = html.cssselect('td.subtext')\n\n                page_stories = dict([ parse_story(*s) for s in zip(title[1::2], subtext) ])\n                try:\n                    next_link = title[-1].xpath('.//a[text() = \"More\"]/@href')\n                except IndexError:\n                    sys.exit(\"Caught IndexError. Dumping HTML:\" + lxml.html.tostring(html))\n\n                next_link = next_link[0] if next_link else None\n\n                stories.update(page_stories)\n\n                should_break = (break_func and break_func(page_stories)) or next_link is None\n                if should_break:\n                    break\n\n                url = self.resolve_url(next_link)\n                page += 1\n\n                logger.info(\"  Sleeping (1s)...\")\n                sleep(1)\n            except Exception as e:\n                if html:\n                    logger.debug(\"Caught exception. Dumping page...\")\n                    logger.debug(\"______________\")\n                    logger.debug(lxml.html.tostring(html, pretty_print=True))\n                    logger.debug(\"______________\")\n                raise\n\n        logger.info(\"Done.\")\n        return stories\n",
  "line_no": 88,
  "line_no_percent": "44%"
}