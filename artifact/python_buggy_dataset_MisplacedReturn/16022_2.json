{
  "instruction": "Create a Python program to classify a text document into one of five categories (news, sports, health, politics, and entertainment).",
  "buggy_code": "import nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ncategories = [\"news\", \"sports\", \"health\", \"politics\", \"entertainment\"]\ndata = {\n \"news\": [[\"This is a news article.\"], [\"An event occurred in the news today.\"]],\n \"sports\": [[\"A huge basketball game was played today.\"], [\"The Lakers won the basketball game.\"]],\n \"health\": [[\"Someone ate an unhealthy snack today.\"], [\"The doctor said to exercise every day.\"]],\n \"politics\": [[\"The President is holding a meeting today.\"], [\"The senators are discussing a bill.\"]],\n \"entertainment\": [[\"A new movie came out this week.\"], [\"A famous celebrity was seen at the premiere.\"]]\n}\n\ndef tokenize_doc(doc):\n tokens = word_tokenize(doc)\n tokens = [word.lower() for word in tokens if word not in string.punctuation]\n tokens = [word for word in tokens if word not in stopwords.words('english')]\n return tokens\n\ndef vectorize_tokens(tokens):\n return\n vec = {}\n for token in tokens:\n if token in vec:\n vec[token] += 1\n else:\n vec[token] = 1\n return vec\n\ntraining_data = [(vectorize_tokens(tokenize_doc(doc)), category)\n for category in data\n for doc in data[category]]\n\ndef classify_doc(doc):\n tokens = tokenize_doc(doc)\n vec = vectorize_tokens(tokens)\n best_category = None\n best_score = 0\n for (category, v) in training_data:\n score = 0\n for token, count in vec.items():\n if token in v:\n score += v[token]\n if score > best_score:\n best_score = score\n best_category = category\n return best_category\n\ntext = \"A famous celebrity was seen today.\"\n \npredicted_category = classify_doc(text)\nprint(\"Predicted category:\", predicted_category)\n",
  "line_no": 22,
  "line_no_percent": "42%"
}