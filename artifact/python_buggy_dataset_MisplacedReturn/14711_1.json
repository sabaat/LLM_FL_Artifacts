{
  "instruction": "This code defines a Python function called `dd_hyperbolic_single` that runs a Bayesian analysis of the Delay Discounting Task using the Hyperbolic Model. The function takes in a data set in the form of a Pandas DataFrame, a file path, or the string \"example\", and returns an `hbayesdm.TaskModel` instance with various components, including the fitted model and raw data. The function has several optional arguments that allow for customization of the analysis, such as the number of iterations, number of chains, and initial values. To use the function, call `dd_hyperbolic_single()` with the desired arguments.",
  "buggy_code": "from typing import Sequence, Union, Any\nfrom collections import OrderedDict\n\nfrom numpy import Inf, exp\nimport pandas as pd\n\nfrom hbayesdm.base import TaskModel\nfrom hbayesdm.preprocess_funcs import dd_single_preprocess_func\n\n__all__ = ['dd_hyperbolic_single']\n\n\nclass DdHyperbolicSingle(TaskModel):\n    def __init__(self, **kwargs):\n        return\n        super().__init__(\n            task_name='dd',\n            model_name='hyperbolic',\n            model_type='single',\n            data_columns=(\n                'subjID',\n                'delay_later',\n                'amount_later',\n                'delay_sooner',\n                'amount_sooner',\n                'choice',\n            ),\n            parameters=OrderedDict([\n                ('k', (0, 0.1, 1)),\n                ('beta', (0, 1, 5)),\n            ]),\n            regressors=OrderedDict([\n                \n            ]),\n            postpreds=['y_pred'],\n            parameters_desc=OrderedDict([\n                ('k', 'discounting rate'),\n                ('beta', 'inverse temperature'),\n            ]),\n            additional_args_desc=OrderedDict([\n                \n            ]),\n            **kwargs,\n        )\n\n    _preprocess_func = dd_single_preprocess_func\n\n\ndef dd_hyperbolic_single(\n        data: Union[pd.DataFrame, str, None] = None,\n        niter: int = 4000,\n        nwarmup: int = 1000,\n        nchain: int = 4,\n        ncore: int = 1,\n        nthin: int = 1,\n        inits: Union[str, Sequence[float]] = 'vb',\n        ind_pars: str = 'mean',\n        model_regressor: bool = False,\n        vb: bool = False,\n        inc_postpred: bool = False,\n        adapt_delta: float = 0.95,\n        stepsize: float = 1,\n        max_treedepth: int = 10,\n        **additional_args: Any) -> TaskModel:\n    \"\"\"Delay Discounting Task - Hyperbolic Model\n\n    Individual Bayesian Modeling of the Delay Discounting Task \n    using Hyperbolic Model [Mazur1987]_ with the following parameters:\n    \"k\" (discounting rate), \"beta\" (inverse temperature).\n\n    \n\n    \n    .. [Mazur1987] Mazur, J. E. (1987). An adjustment procedure for studying delayed reinforcement.\n\n    \n\n    User data should contain the behavioral data-set of all subjects of interest for\n    the current analysis. When loading from a file, the datafile should be a\n    **tab-delimited** text file, whose rows represent trial-by-trial observations\n    and columns represent variables.\n\n    For the Delay Discounting Task, there should be 6 columns of data\n    with the labels \"subjID\", \"delay_later\", \"amount_later\", \"delay_sooner\", \"amount_sooner\", \"choice\". It is not necessary for the columns to be\n    in this particular order; however, it is necessary that they be labeled\n    correctly and contain the information below:\n\n    - \"subjID\": A unique identifier for each subject in the data-set.\n    - \"delay_later\": An integer representing the delayed days for the later option (e.g. 1, 6, 28).\n    - \"amount_later\": A floating point number representing the amount for the later option (e.g. 10.5, 13.4, 30.9).\n    - \"delay_sooner\": An integer representing the delayed days for the sooner option (e.g. 0).\n    - \"amount_sooner\": A floating point number representing the amount for the sooner option (e.g. 10).\n    - \"choice\": If amount_later was selected, choice == 1; else if amount_sooner was selected, choice == 0.\n\n    .. note::\n        User data may contain other columns of data (e.g. ``ReactionTime``,\n        ``trial_number``, etc.), but only the data within the column names listed\n        above will be used during the modeling. As long as the necessary columns\n        mentioned above are present and labeled correctly, there is no need to\n        remove other miscellaneous data columns.\n\n    .. note::\n\n        ``adapt_delta``, ``stepsize``, and ``max_treedepth`` are advanced options that\n        give the user more control over Stan's MCMC sampler. It is recommended that\n        only advanced users change the default values, as alterations can profoundly\n        change the sampler's behavior. See [Hoffman2014]_ for more information on the\n        sampler control parameters. One can also refer to 'Section 34.2. HMC Algorithm\n        Parameters' of the `Stan User's Guide and Reference Manual`__.\n\n        .. [Hoffman2014]\n            Hoffman, M. D., & Gelman, A. (2014).\n            The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.\n            Journal of Machine Learning Research, 15(1), 1593-1623.\n\n        __ https://mc-stan.org/users/documentation/\n\n    Parameters\n    ----------\n    data\n        Data to be modeled. It should be given as a Pandas DataFrame object,\n        a filepath for a data file, or ``\"example\"`` for example data.\n        Data columns should be labeled as: \"subjID\", \"delay_later\", \"amount_later\", \"delay_sooner\", \"amount_sooner\", \"choice\".\n    niter\n        Number of iterations, including warm-up. Defaults to 4000.\n    nwarmup\n        Number of iterations used for warm-up only. Defaults to 1000.\n\n        ``nwarmup`` is a numerical value that specifies how many MCMC samples\n        should not be stored upon the beginning of each chain. For those\n        familiar with Bayesian methods, this is equivalent to burn-in samples.\n        Due to the nature of the MCMC algorithm, initial values (i.e., where the\n        sampling chains begin) can have a heavy influence on the generated\n        posterior distributions. The ``nwarmup`` argument can be set to a\n        higher number in order to curb the effects that initial values have on\n        the resulting posteriors.\n    nchain\n        Number of Markov chains to run. Defaults to 4.\n\n        ``nchain`` is a numerical value that specifies how many chains (i.e.,\n        independent sampling sequences) should be used to draw samples from\n        the posterior distribution. Since the posteriors are generated from a\n        sampling process, it is good practice to run multiple chains to ensure\n        that a reasonably representative posterior is attained. When the\n        sampling is complete, it is possible to check the multiple chains for\n        convergence by running the following line of code:\n\n        .. code:: python\n\n            output.plot(type='trace')\n    ncore\n        Number of CPUs to be used for running. Defaults to 1.\n    nthin\n        Every ``nthin``-th sample will be used to generate the posterior\n        distribution. Defaults to 1. A higher number can be used when\n        auto-correlation within the MCMC sampling is high.\n\n        ``nthin`` is a numerical value that specifies the \"skipping\" behavior\n        of the MCMC sampler. That is, only every ``nthin``-th sample is used to\n        generate posterior distributions. By default, ``nthin`` is equal to 1,\n        meaning that every sample is used to generate the posterior.\n    inits\n        String or list specifying how the initial values should be generated.\n        Options are ``'fixed'`` or ``'random'``, or your own initial values.\n    ind_pars\n        String specifying how to summarize the individual parameters.\n        Current options are: ``'mean'``, ``'median'``, or ``'mode'``.\n    model_regressor\n        Whether to export model-based regressors. Currently not available for this model.\n    vb\n        Whether to use variational inference to approximately draw from a\n        posterior distribution. Defaults to ``False``.\n    inc_postpred\n        Include trial-level posterior predictive simulations in\n        model output (may greatly increase file size). Defaults to ``False``.\n    adapt_delta\n        Floating point value representing the target acceptance probability of a new\n        sample in the MCMC chain. Must be between 0 and 1. See note below.\n    stepsize\n        Integer value specifying the size of each leapfrog step that the MCMC sampler\n        can take on each new iteration. See note below.\n    max_treedepth\n        Integer value specifying how many leapfrog steps the MCMC sampler can take\n        on each new iteration. See note below.\n    **additional_args\n        Not used for this model.\n\n    Returns\n    -------\n    model_data\n        An ``hbayesdm.TaskModel`` instance with the following components:\n\n        - ``model``: String value that is the name of the model ('dd_hyperbolic_single').\n        - ``all_ind_pars``: Pandas DataFrame containing the summarized parameter values\n          (as specified by ``ind_pars``) for each subject.\n        - ``par_vals``: OrderedDict holding the posterior samples over different parameters.\n        - ``fit``: A PyStan StanFit object that contains the fitted Stan model.\n        - ``raw_data``: Pandas DataFrame containing the raw data used to fit the model,\n          as specified by the user.\n        \n\n    Examples\n    --------\n\n    .. code:: python\n\n        from hbayesdm import rhat, print_fit\n        from hbayesdm.models import dd_hyperbolic_single\n\n        output = dd_hyperbolic_single(data='example', niter=2000, nwarmup=1000, nchain=4, ncore=4)\n\n        output.plot(type='trace')\n\n        output.plot()\n\n        rhat(output, less=1.1)\n\n        print_fit(output)\n    \"\"\"\n    return DdHyperbolicSingle(\n        data=data,\n        niter=niter,\n        nwarmup=nwarmup,\n        nchain=nchain,\n        ncore=ncore,\n        nthin=nthin,\n        inits=inits,\n        ind_pars=ind_pars,\n        model_regressor=model_regressor,\n        vb=vb,\n        inc_postpred=inc_postpred,\n        adapt_delta=adapt_delta,\n        stepsize=stepsize,\n        max_treedepth=max_treedepth,\n        **additional_args)\n",
  "line_no": 15,
  "line_no_percent": "6%"
}