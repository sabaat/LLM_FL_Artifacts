{
  "instruction": "Design and implement an artificial intelligence agent in Python using the Q-learning algorithm to solve a gridworld environment with 8x16 cells.",
  "buggy_code": "import numpy as np \n\nclass GridWorld:\n  def __init__(self, rows, cols, start):\n    self.rows = rows\n    self.cols = cols\n    self.i = start[0]\n    self.j = start[1]\n    \n  def set(self, rewards, actions):\n    self.rewards = rewards\n    self.actions = actions\n    \n  def set_state(self, s):    \n    self.i = s[0]\n    self.j = s[1]\n    \n  def current_state(self):\n    return (self.i, self.j)\n    \n  def is_terminal(self, s):\n    return s not in self.actions\n    \n  def move(self, action):\n    if action in self.actions[(self.i, self.j)]:\n      if action == 'U':\n        self.i -= 1\n      elif action == 'D':\n        self.i += 1\n      elif action == 'R':\n        self.j += 1\n      elif action == 'L':\n        self.j -= 1\n    return self.rewards.get((self.i, self.j), 0)\n    \n  def undo_move(self, action):\n    if action == 'U':\n      self.i += 1\n    elif action == 'D':\n      self.i -= 1\n    elif action == 'R':\n      self.j -= 1\n    elif action == 'L':\n      self.j += 1\n    assert(self.current_state() in self.all_states())\n    \n  def game_over(self):\n    return (self.i, self.j) not in self.actions\n  \n  def all_states(self):\n    return set(self.actions.keys()) | set(self.rewards.keys())\n    \ndef standard_grid():\n    return\n    g = GridWorld(3,4,(2,0))\n    rewards = {(0,3):1, (1,3):-1}\n    actions = {\n        (0, 0): ('D', 'R'),\n        (0, 1): ('L', 'R'),\n        (0, 2): ('L', 'D', 'R'),\n        (1, 0): ('U', 'D'),\n        (1, 2): ('U', 'D', 'R'),\n        (2, 0): ('U', 'R'),\n        (2, 1): ('L', 'R'),\n        (2, 2): ('L', 'R', 'U'),\n        (2, 3): ('L', 'U'),\n      }\n    g.set(rewards, actions)\n    return g\n\n\ndef max_dict(d):\n    max_key = None\n    max_val = float('-inf')\n    for k, v in d.items():\n        if v > max_val:\n            max_val = v\n            max_key = k\n    return max_key, max_val\n\n\ndef random_action(a, eps=0.1):\n    p = np.random.random()\n    if p < (1 - eps):\n        return a\n    else:\n        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n\ndef q_learning(env, num_episodes, discount_factor=1.0, alpha=0.7, epsilon=0.1):  \n\n    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n\n    stats = plotting.EpisodeStats(\n        episode_lengths=np.zeros(num_episodes),\n        episode_rewards=np.zeros(num_episodes))    \n\n    for i_episode in range(num_episodes):\n\n        if (i_episode + 1) % 100 == 0:\n            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n            sys.stdout.flush()\n\n        state = env.reset()\n        action = random_action(env.action_space.sample(), epsilon)\n\n        for t in itertools.count():\n\n            next_state, reward, done, _ = env.step(action)\n\n            next_action = random_action(env.action_space.sample(), epsilon)\n\n            stats.episode_rewards[i_episode] += reward\n            stats.episode_lengths[i_episode] = t\n\n            best_next_action = np.argmax(Q[next_state])  \n            td_target = reward + discount_factor * Q[next_state][best_next_action]\n            td_delta = td_target - Q[state][action]\n            Q[state][action] += alpha * td_delta\n\n            if done:\n                break\n\n            action = next_action\n            state = next_state\n\n    return Q, stats\n\nQ, stats = q_learning(env, 500)\n\npolicy = np.array([np.argmax(Q[key]) if key in Q else -1 for key in np.arange(48)]).reshape((8,16))\n\nV = np.array([np.max(Q[key]) if key in Q else 0 for key in np.arange(48)]).reshape((8,16))\n\nprint(\"\\nPolicy (UP=0, DOWN=1, LEFT=2, RIGHT=3):\")\nprint(policy)\n",
  "line_no": 54,
  "line_no_percent": "40%"
}