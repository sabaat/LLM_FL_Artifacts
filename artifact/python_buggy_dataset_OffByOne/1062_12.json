{
  "instruction": "This code is a Python script that defines a class called \"source\" with several methods for searching and retrieving movie and TV show sources from the website filmpalast.to. To use this code, import the \"source\" class and create an instance of it, then call its methods to search for and retrieve sources for specific movies or TV shows.",
  "buggy_code": "import json\nimport re\nimport urllib\nimport urlparse\nfrom openscrapers.modules import cleantitle\nfrom openscrapers.modules import client\nfrom openscrapers.modules import dom_parser\nfrom openscrapers.modules import source_utils\n\nclass source:\n\n    def __init__(self):\n        self.priority = 1\n        self.language = ['de']\n        self.domains = ['filmpalast.to']\n        self.base_link = 'http://filmpalast.to'\n        self.search_link = '/search/title/%s'\n        self.stream_link = 'stream/%s/1'\n\n    def movie(self, imdb, title, localtitle, aliases, year):\n        try:\n            url = self.__search([localtitle] + source_utils.aliases_to_array(aliases))\n            if not url and title != localtitle:\n                url = self.__search([title] + source_utils.aliases_to_array(aliases))\n            return url\n        except:\n            return\n\n    def tvshow(self, imdb, tvdb, tvshowtitle, localtvshowtitle, aliases, year):\n        try:\n            url = {'imdb': imdb, 'tvdb': tvdb, 'tvshowtitle': tvshowtitle, 'localtvshowtitle': localtvshowtitle, 'aliases': aliases, 'year': year}\n            url = urllib.urlencode(url)\n            return url\n        except:\n            return\n\n    def episode(self, url, imdb, tvdb, title, premiered, season, episode):\n        try:\n            if not url:\n                return\n            data = urlparse.parse_qs(url)\n            data = dict([(i, data[i][0]) if data[i] else (i, '') for i in data])\n            title = data['localtvshowtitle']\n            title += ' S%02dE%02d' % (int(season), int(episode))\n            aliases = source_utils.aliases_to_array(eval(data['aliases']))\n            aliases = [i + ' S%02dE%02d' % (int(season), int(episode)) for i in aliases]\n            url = self.__search([title] + aliases)\n            if not url and data['tvshowtitle'] != data['localtvshowtitle']:\n                title = data['tvshowtitle']\n                title += ' S%02dE%02d' % (int(season), int(episode))\n                url = self.__search([title] + aliases)\n            return url\n        except:\n            return\n\n    def sources(self, url, hostDict, hostprDict):\n        sources = []\n        try:\n            if not url:\n                return sources\n            query = urlparse.urljoin(self.base_link, url)\n            r = client.request(query)\n            quality = dom_parser.parse_dom(r, 'span', attrs={'id': 'release_text'})[0].content.split('&nbsp;')[0]\n            quality, info = source_utils.get_release_quality(quality)\n            r = dom_parser.parse_dom(r, 'ul', attrs={'class': 'currentStreamLinks'})\n            r = [(dom_parser.parse_dom(i, 'p', attrs={'class': 'hostName'}), dom_parser.parse_dom(i, 'a', attrs={'class': 'stream-src'}, req='data-id')) for i in r]\n            r = [(re.sub(' hd$', '', i[0][1].content.lower()), [x.attrs['data-id'] for x in i[1]]) for i in r if i[0] and i[1]]\n            for hoster, id in r:\n                valid, hoster = source_utils.is_host_valid(hoster, hostDict)\n                if not valid:\n                    continue\n                sources.append({'source': hoster, 'quality': quality, 'language': 'de', 'info': ' | '.join(info + ['' if len(id) == 1 else 'multi-part']), 'url': id, 'direct': False, 'debridonly': False, 'checkquality': True})\n            return sources\n        except:\n            return sources\n\n    def resolve(self, url):\n        try:\n            h_url = []\n            for id in url:\n                query = urlparse.urljoin(self.base_link, self.stream_link % id)\n                r = client.request(query, XHR=True, post=urllib.urlencode({'streamID': id}))\n                r = json.loads(r)\n                if 'error' in r and r['error'] == '0' and ('url' in r):\n                    h_url.append(r['url'])\n            h_url = h_url[0] if len(h_url) == 1 else 'stack://' + ' , '.join(h_url)\n            return h_url\n        except:\n            return\n\n    def __search(self, titles):\n        try:\n            query = self.search_link % urllib.quote_plus(titles[0])\n            query = urlparse.urljoin(self.base_link, query)\n            t = [cleantitle.get(i) for i in set(titles) if i]\n            r = client.request(query)\n            r = dom_parser.parse_dom(r, 'article')\n            r = dom_parser.parse_dom(r, 'a', attrs={'class': 'rb'}, req='href')\n            r = [(i.attrs['href'], i.content) for i in r]\n            r = [i[0] for i in r if cleantitle.get(i[1]) in t][0]\n            return source_utils.strip_domain(r)\n        except:\n            return",
  "line_no": 76,
  "line_no_percent": "74%"
}