{
  "instruction": "Create a question-answering system using Python and Natural Language Processing.",
  "buggy_code": "import nltk, numpy\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\nimport json\nwith open('intents.json') as json_data:\n    intents = json.load(json_data)\nwords = []\nclasses = []\ndocuments = []\nignore_words = ['?']\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        w = nltk.word_tokenize(pattern)\n        words.extend(w)\n        documents.append((w, intent['tag']))\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\nwords = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\nwords = sorted(list(set(words)))\nclasses = sorted(list(set(classes)))\ntraining = []\noutput = []\noutput_empty = [0] * len(classes)\nfor doc in documents:\n    bag = []\n    pattern_words = doc[0]\n    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n    for w in words:\n        bag.append(1) if w in pattern_words else bag.append(0)\n    training.append(bag)\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n    output.append(output_row)\ni = 0\nw = documents[i][0]\nprint([stemmer.stem(word.lower()) for word in w])\nprint(training[i])\nprint(output[i])\nimport numpy\nimport tflearn\nimport tensorflow as tf\nimport random\nrandom.seed(1)\ntraining = numpy.array(training)\noutput = numpy.array(output)\nnet = tflearn.input_data(shape=[None, len(training[0])])\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, len(output[1]), activation='softmax')\nnet = tflearn.regression(net)\nmodel = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\nmodel.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\nmodel.save('model.tflearn')\nimport pickle\npickle.dump({'words': words, 'classes': classes, 'train_x': training, 'train_y': output}, open('training_data', 'wb'))",
  "line_no": 59,
  "line_no_percent": "107%"
}